# -*- coding: utf-8 -*-
"""hw07_5_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iC81yR5I8eehL_EkDLJnQ-giShcsM-Yp
"""

!pip install openai
!pip install datasets
!pip transformers

from datasets import load_dataset
import random
dataset = load_dataset("boolq")

total = len(dataset['train'])
trueCount = 0
falseCount = 0
prompt = ""
while trueCount != 4 or falseCount != 4:
  index = random.randrange(total)
  if trueCount < 4 and dataset['train']['answer'][index]:
    prompt += dataset['train']['question'][index] + '\n' + \
              dataset['train']['passage'][index] + '\n' + \
              str(dataset['train']['answer'][index]) + '\n\n'
    trueCount += 1
  elif falseCount < 4 and not dataset['train']['answer'][index]:
    prompt += dataset['train']['question'][index] + '\n' + \
              dataset['train']['passage'][index] + '\n' + \
              str(dataset['train']['answer'][index]) + '\n\n'
    falseCount += 1
print(f'true: {trueCount}, false: {falseCount}')

print(prompt)

import openai

modelName = 'text-davinci-003'
result = {}
totalTest = 30
totalCorrect = 0

openai.api_key = ""

for i in range(totalTest):
  newPrompt = prompt + dataset['validation']['question'][i] + '\n' + \
              dataset['validation']['passage'][i] + '\n'
  response = openai.Completion.create(
  model=modelName,
  prompt=newPrompt,
  temperature=0.0,
  max_tokens=256,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0)

  result["label"] = dataset['validation']['answer'][i]
  result["response"] = response['choices'][0]['text']
  totalCorrect += str(result["label"]) == result["response"]
  print('label: ', dataset['validation']['answer'][i], ' gpt: ', response['choices'][0]['text'])

print(totalCorrect/totalTest)

"""5.5 BLOOMZ"""

total = len(dataset['train'])
trueCount = 0
falseCount = 0
prompt = ""
while trueCount != 3 or falseCount != 3:
  index = random.randrange(total)
  if trueCount < 3 and dataset['train']['answer'][index]:
    prompt += dataset['train']['question'][index] + '\n' + \
              dataset['train']['passage'][index] + '\n' + \
              str(dataset['train']['answer'][index]) + '\n\n'
    trueCount += 1
  elif falseCount < 3 and not dataset['train']['answer'][index]:
    prompt += dataset['train']['question'][index] + '\n' + \
              dataset['train']['passage'][index] + '\n' + \
              str(dataset['train']['answer'][index]) + '\n\n'
    falseCount += 1

import requests

API_URL = "https://api-inference.huggingface.co/models/bigscience/bloomz"
headers = {"Authorization": f"Bearer {'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'}"}

def query(input):
  response = requests.post(API_URL, headers=headers, json={"inputs": input,})
  response = response.json()
  # return response
  return response[0]['generated_text'][len(input):]

result = {}
totalTest = 40
totalCorrect = 0
for i in range(totalTest):
  newPrompt = prompt + dataset['validation']['question'][i] + '\n' + \
              dataset['validation']['passage'][i] + '\n'
  response = query(newPrompt)

  result["label"] = dataset['validation']['answer'][i]
  result["response"] = response
  totalCorrect += str(result["label"]) == result["response"]
  # print(newPrompt)
  print('label: ', dataset['validation']['answer'][i], ' Bloomz: ', response)

print(totalCorrect/totalTest)

"""5.6 PETALS"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q petals

from datasets import load_dataset
import torch
from transformers import BloomTokenizerFast 
from petals import DistributedBloomForCausalLM

dataset = load_dataset("boolq")

MODEL_NAME = "bigscience/bloom-petals"
tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)
model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME)
model = model.cuda()

total = len(dataset['train'])
trueCount = 0
falseCount = 0
prompt = ""
while trueCount != 4 or falseCount != 4:
  index = random.randrange(total)
  if trueCount < 4 and dataset['train']['answer'][index]:
    prompt += dataset['train']['question'][index] + '\n' + \
              dataset['train']['passage'][index] + '\n' + \
              str(dataset['train']['answer'][index]) + '\n\n'
    trueCount += 1
  elif falseCount < 4 and not dataset['train']['answer'][index]:
    prompt += dataset['train']['question'][index] + '\n' + \
              dataset['train']['passage'][index] + '\n' + \
              str(dataset['train']['answer'][index]) + '\n\n'
    falseCount += 1
print(f'true: {trueCount}, false: {falseCount}')

result = {}
totalTest = 30
totalCorrect = 0
for i in range(totalTest):
  newPrompt = prompt + dataset['validation']['question'][i] + '\n' + \
              dataset['validation']['passage'][i] + '\n'
  inputs = tokenizer(newPrompt, return_tensors="pt")["input_ids"].cuda()
  outputs = model.generate(inputs, max_new_tokens=1)
  response = tokenizer.decode(outputs[0,-1])
  result["label"] = dataset['validation']['answer'][i]
  result["response"] = response
  totalCorrect += str(result["label"]) == result["response"]
  print('label: ', dataset['validation']['answer'][i], ' petals: ', response)

print(totalCorrect/totalTest)

"""Fine-tune Petals"""

class BoolQADataset(torch.utils.data.Dataset):
    """
    Dataset for the dataset of BoolQ questions and answers
    """

    def __init__(self, passages, questions, answers, tokenizer, max_len):
        self.passages = passages
        self.questions = questions
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.answers)

    def __getitem__(self, index):
        """
        This function is called by the DataLoader to get an instance of the data
        :param index:
        :return:
        """

        passage = str(self.passages[index])
        question = self.questions[index]
        answer = self.answers[index]

        # this is input encoding for your model. Note, question comes first since we are doing question answering
        # and we don't want it to be truncated if the passage is too long
        input_encoding = question + " [SEP] " + passage

        # encode_plus will encode the input and return a dictionary of tensors
        encoded_review = self.tokenizer.encode_plus(
            input_encoding,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            return_attention_mask=True,
            return_tensors="pt",
            padding="max_length",
            truncation=True
        )

        return {
            'input_ids': encoded_review['input_ids'][0],  # we only have one example in the batch
            'attention_mask': encoded_review['attention_mask'][0],
            # attention mask tells the model where tokens are padding
            'labels': torch.tensor(answer, dtype=torch.long)  # labels are the answers (yes/no)
        }

import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from datasets import load_dataset
from petals import DistributedBloomForCausalLM

device = torch.device('cuda:0')
# Load the BoolQ dataset
dataset_train_subset = dataset['train'].shuffle(seed=42).select(range(50))

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-petals")
max_len = 512

# Create the BoolQADataset instance
train_dataset = BoolQADataset(
    passages=list(dataset_train_subset['passage']),
    questions=list(dataset_train_subset['question']),
    answers=list(dataset_train_subset['answer']),
    tokenizer=tokenizer,
    max_len=max_len
)

# Create the DataLoader
data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Load the Petals model
model = DistributedBloomForCausalLM.from_pretrained(
    "bigscience/bloom-petals", tuning_mode="ptune", pre_seq_len=16
)

model = model.to(device)

# Fine-tuning (updates only prompts or adapters hosted locally)
optimizer = torch.optim.AdamW(model.parameters())
cross_entropy = torch.nn.CrossEntropyLoss()

for batch in data_loader:
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]

    input_ids = input_ids.to(device)
    labels = labels.to(device)

    outputs = model(input_ids)
    logits = outputs.logits[:, -1, :]  # Get the logits for the last token
    loss = cross_entropy(logits, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()